---
layout: post
title:  "Strategies for Enhancing Topic Coherence and Diversity through QualIT"
---

<div style="font-size: 36px; text-align: center;">
    Source
</div>

<p style="text-align: center;">
  ðŸ“„ <a href="https://github.com/HeeseungMoon/HeeseungMoon.github.io/raw/master/assets/(2024) Kapoor et al.pdf" target="_blank">(2024) Kapoor et al.pdf.</a>
</p>


<div style="font-size: 36px; text-align: center;">
  <b>1. Summary of the reading</b>
</div>

<div style="text-align: center;">
- This paper proposes Qualitative Insights Tool (QualIT), a novel tool based on Large Language Models (LLM) to overcome the limitations of traditional topic modeling techniques (LDAs, etc.).
</div>

<div style="text-align: center;">
- The tool combines the contextual comprehension and clustering techniques of LLM to derive more meaningful, consistent, and diverse topics from complex unstructured texts such as news articles or Voice of Customer. 
</div>

<div style="text-align: center;">
- It outperforms existing methods in topic coherence and topic diversity, and demonstrates that it can significantly reduce analysis time in human resource management and qualitative research.
</div>

<br>

<div style="font-size: 18px; text-align: center;">
  <b> â€¢ Research Question & Research Gap</b>
</div>

<div style="text-align: center;">
- Research Question:
</div>

<div style="text-align: center;">
- How can large language models (LLMs) be integrated with clustering techniques to improve the coherence and diversity of topic modeling from unstructured text data, particularly in talent management research?
</div>

<br>

<div style="text-align: center;">
- Research Gap:
</div>

<div style="text-align: center;">
- Traditional methods such as LDA and BERTOPIC are:
</div>


<div style="text-align: center;">
1. Lacking contextual understanding
</div>

<div style="text-align: center;">
2. Difficulty extracting multiple topics from one document
</div>

<div style="text-align: center;">
3. Have to manually set the number of clusters
</div>


<div style="text-align: center;">
4. Sensitive to noise
</div>


<div style="text-align: center;">
- Therefore, it was necessary to overcome existing limitations and to have more precise and interpretable topic modeling tools.
</div>

<br>

<div style="font-size: 18px; text-align: center;">
  <b> â€¢ Method</b>
</div>



<div style="text-align: center;">
- There are 3 methods.
</div>

<div style="text-align: center;">
1. Key Phrase Extraction
</div>

<div style="text-align: center;">
- Using Cloud 2.1 based LLM, key phrases are extracted from each document.
</div>

<div style="text-align: center;">
- This is more favorable than BERTopic, reflecting that there may be multiple topics in a document.
</div>

<br>

<div style="text-align: center;">
2. Hallucination Check
</div>

<div style="text-align: center;">
- The coherence score was calculated to check the reliability of the extracted keyphrase.
</div>


<div style="text-align: center;">
- If the score is less than 10%, it is considered 'hallucination' and removed.
</div>

<br>

<div style="text-align: center;">
3. Clustering
</div>

<div style="text-align: center;">
- Apply K-means clustering in two stages:
</div>


<div style="text-align: center;">
- Step 1: Cluster main topics
</div>


<div style="text-align: center;">
- Step 2: Cluster sub-topics within each topic
Automatic discovery of optimal number of clusters with the Silhouette score.
</div>

<br>

<div style="font-size: 18px; text-align: center;">
  <b> â€¢ Results + Interpretation</b>
</div>



| Model     | Topic Coherence (20 topics) | Topic Diversity (20 topics) |
|-----------|------------------------------|------------------------------|
| LDA       | 57%                          | 72%                          |
| BERTopic  | 65%                          | 85%                          |
| **QualIT** | **70%**                     | **95.5%**                    |


<div style="text-align: center;">
- Human Evaluation:
</div>

<div style="text-align: center;">
- All evaluators match rate:
</div>

<div style="text-align: center;">
- LDA: 20%
</div>

<div style="text-align: center;">
- BERTopic: 20%
</div>

<div style="text-align: center;">
- QualIT: 35%
</div>

<br>

<div style="text-align: center;">
- Interpretation:
</div>

<div style="text-align: center;">
- QualIT generates clearer and interpretable topics by considering both semantic similarity and diversity between words, and has a high rate of classification agreement between people â†’ It is useful in real-world research sites.
</div>

<br>

<div style="font-size: 36px; text-align: center;">
  <b>2. Data introduction</b>
</div>

<div style="text-align: center;">
- I used the data from https://cseweb.ucsd.edu/~jmcauley/datasets.html#amazon_reviews.
</div>

<div style="text-align: center;">
- This analysis was based on Amazon Product Data, including user reviews collected by Amazon between the "Video Games" categories 2013 and 2014.
</div>

<br>

<div style="font-size: 18px; text-align: center;">
  <b> â€¢ Major characteristics of the data</b>
</div>

<div style="text-align: center;">
1. Text-centric data
</div>

<div style="text-align: center;">
- The key analysis targets are reviewText and summary, suitable for NLP-based analysis. 
</div>

<div style="text-align: center;">
- NLP-based analysis
</div>

<div style="text-align: center;">
- Review length varies from a short line to hundreds of words.
</div>

<div style="text-align: center;">
2. applicability
</div>

<div style="text-align: center;">
- This data is suitable for the following tasks:
</div>

<div style="text-align: center;">
- Sentiment Analysis, Topic Modeling, Keyphrase Extraction, LLM-based summary and qualitative analysis like GPT.
</div>

<br>

<div style="font-size: 18px; text-align: center;">
  <b> â€¢ Data pre-processing</b>
</div>

<br>

```python
df_2_star = df[(df['rating'] == 2) & (df['text'].str.len() > 20) & (df['text'].str.len() < 200) & df['helpful_vote'] > 0]
```


<div style="text-align: center;">
- Select only reviews with 2 stars (underrated reviews)
</div>

<div style="text-align: center;">
- Except for too short reviews (reviews with little content)
</div>

<div style="text-align: center;">
- Excluding reviews that are too long (LLM processing burden)
</div>

<div style="text-align: center;">
- Select only reviews that others rated as 'helpful'
</div>

<div style="text-align: center;">
- Choose only meaningful complaint reviews and use them for analysis
</div>


<div style="text-align: center;">
- Text length limitation to maintain the appropriate length for LLM input
</div>


<div style="text-align: center;">
- Remove noise by choosing only reviews with higher vote.
</div>

<div style="font-size: 36px; text-align: center;">
  <b>3. Analysis</b>
</div>


<div style="font-size: 18px; text-align: center;">
  <b> â€¢ LDA Model</b>
</div>

<div style="text-align: center;">
<b>- Text preprocessing</b>
</div>

```python
def preprocess_text(text):
    text = re.sub('\s+', ' ', str(text))  
    text = re.sub('\S*@\S*\s?', '', str(text))  
    text = re.sub('\'', '', str(text))  
    text = re.sub('[^a-zA-Z]', ' ', str(text))  
    text = text.lower()  
    return text

df_2_star['cleaned_text'] = df_2_star['text'].apply(preprocess_text)
```

<div style="text-align: center;">
What it does:
</div>

<div style="text-align: center;">
- Remove email address
</div>

<div style="text-align: center;">
- Remove apostrophe
</div>

<div style="text-align: center;">
- Remove non-alpha characters
</div>

<div style="text-align: center;">
- Converting to lowercase
</div>

<div style="text-align: center;">
- Clean up text and save it in the 'cleaned_text' column
</div>

<br>

<div style="text-align: center;">
- This is to eliminate noise before machine learning/topic modeling and unify it in a format suitable for analysis.
</div>

<br>

<div style="text-align: center;">
<b>- Tokenize and remove the stopwords</b>
</div>


```python
nltk.download('stopwords')
stop_words = stopwords.words('english')

def tokenize(text):
    tokens = simple_preprocess(text, deacc=True)
    tokens = [token for token in tokens if token not in stop_words]
    return tokens
```

<br>

<div style="text-align: center;">
- Detach tokens with simple_preprocess()
</div>

<div style="text-align: center;">
- Get and remove English stopwords from NLTK
</div>

<div style="text-align: center;">
- Perform to remove meaningless words (for example, the, and, to) from the analysis and to leave only the key words.
</div>

<br>

<div style="text-align: center;">
<b>- Lemmatization</b>
</div>

```python
try:
    nltk.data.find('corpora/wordnet')
except LookupError:
    nltk.download('wordnet')


lemmatizer = WordNetLemmatizer()

def extract_lemmas(tokens):
    return [lemmatizer.lemmatize(token) for token in tokens]
```

<div style="text-align: center;">
- Use WordNet Lemmatizer to convert tokens into headings (default).
</div>

<div style="text-align: center;">
- This is to unify the shape of the word and understand the subject more accurately.
</div>

<br>

<div style="text-align: center;">
<b>- Create dictionary and corpus</b>
</div>

```python
id2word = corpora.Dictionary(df_2_star['lemmas'])
texts = df_2_star['lemmas']
corpus = [id2word.doc2bow(text) for text in texts]
```

<div style="text-align: center;">
- Create a dictionary that maps word IDs to real words
</div>

<div style="text-align: center;">
- The corpus is an LDA input format in which each document is converted into a tuple (word ID, frequency)
</div>

<div style="text-align: center;">
- This is because the LDA model requires a number-based corpus and word ID.
</div>

<br>

<div style="text-align: center;">
<b>- LDA Model Learning</b>
</div>

```python
lda_model = LdaModel(corpus=corpus,
                     id2word=id2word,
                     num_topics=10,
                     random_state=100,
                     update_every=1,
                     chunksize=100,
                     passes=10,
                     alpha='auto',
                     per_word_topics=True)
```

<div style="text-align: center;">
- Create an LDA model with num_topics=10
</div>

<div style="text-align: center;">
- Control with parameters such as number of topics, number of iterations, alpha, etc
</div>

<div style="text-align: center;">
- Create a probability distribution of what topics each document belongs to
</div>

<div style="text-align: center;">
- Learn the latent topics of documents, and use them to summarize or categorize them.
</div>

<br>

```python
topics = lda_model.print_topics(num_words=10)
for topic in topics:
    print(topic)
```

<div style="text-align: center;">
- Prints important words (top 10) from each topic
</div>

<div style="text-align: center;">
- This is to help people interpret what each topic means.
</div>

<br>

<div style="text-align: center;">
<b>- coherence score & diversity score</b>
</div>

```python
coherence_model_lda = CoherenceModel(model=lda_model, texts=df_2_star['lemmas'], dictionary=id2word, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)
```

<div style="text-align: center;">
- Measure how semantically similar the words in the topic are.
</div>

<div style="text-align: center;">
- It is used to numerically evaluate how consistent topic words are from a person's point of view.
</div>

![Coherence1](/assets/images/Coherence1.png)

<div style="text-align: center;">
- In this example, it is output as Coherence Score = 0.2985 which is low
</div>

<br>

```python
top_n_words = 20
topic_words = []

for i in range(lda_model.num_topics):
    words_with_probs = lda_model.show_topic(i, topn=top_n_words)
    words = [word for word, prob in words_with_probs]
    topic_words.append(words)

# top n words from every topic to a list
all_words_in_topics = [word for topic in topic_words for word in topic]
total_words_count = len(all_words_in_topics)
unique_words_count = len(set(all_words_in_topics))

# diversity score
diversity_score = unique_words_count / total_words_count

print(f"Total words across all topics: {total_words_count}")
print(f"Unique words across all topics: {unique_words_count}")
print(f"Diversity Score (N={top_n_words}): {diversity_score:.4f}")
```

<br>

<div style="text-align: center;">
- After extracting top-N words from each topic, the proportion of words without duplication in the whole is calculated.
</div>

<div style="text-align: center;">
- Diversity = Number of unique words / Total words
</div>

<div style="text-align: center;">
- By evaluating how different words each topic uses, it is to diagnose the redundancy between topics.
</div>

<br>

![DIversity1](/assets/images/Diversity1.png)

<div style="text-align: center;">
- In this example, Diversity Score = 1.0000 shows very high diversity.
</div>

<br>

<div style="text-align: center;">
<b>Custom tokenizer + vectorizer settings</b>
</div>

```python
def nltk_tokenizer(text):
    text = text.lower()
    tokens = re.findall(r'\b[a-z]+\b', text)  
    return [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]

 vectorizer_model = CountVectorizer(
    tokenizer=nltk_tokenizer,
    stop_words='english',
    min_df=5,
    max_df=0.9,
    ngram_range=(1, 1)
)
```

<div style="text-align: center;">
- The nltk_tokenizer() function does:
</div>

<div style="text-align: center;">
1. Lowercase Transformation
</div>

<div style="text-align: center;">
2. Extract only alphabetic words 
</div>

<div style="text-align: center;">
3. Elimination of non-verbal terms 
</div>

<div style="text-align: center;">
4. Lemmatization
</div>

<div style="text-align: center;">
- CountVectorizer converts text into numeric vectors.
</div>

<div style="text-align: center;">
- This is to define the Bag-of-Words (BoW) representation to be used in BERTopic's clustering process.
</div>

<br>

<div style="text-align: center;">
<b>Generating and learning BERTopic models</b>
</div>

```python
model = BERTopic(
    verbose=True,
    embedding_model='all-MiniLM-L6-v2',
    vectorizer_model=vectorizer_model,
    language='english',
    nr_topics=20,
    top_n_words=20,
    calculate_probabilities=True
)
```

<div style="text-align: center;">
- Generating text embeddings with MiniLM-based embeddings.
</div>

<div style="text-align: center;">
- Dimension with UMAP followed by clustering with HDBSCAN.
</div>

<div style="text-align: center;">
- Extracting topics based on TF-IDF scores in the cluster.
</div>

<div style="text-align: center;">
- It is a state-of-the-art topic modeling combining three techniques: context-based embedding + density clustering + TF-IDF-based subject name extraction.
</div>

<br>

<div style="text-align: center;">
<b>Check topic statistics</b>
</div>

```python
freq = model.get_topic_info()
```

<div style="text-align: center;">
- Outputs such as unique ID, number of documents, topic name, representative word list, representative document, etc. for each topic.
</div>

<div style="text-align: center;">
- Use to interpret the frequency and meaning of each topic.
</div>

<br>

<div style="text-align: center;">
<b>model.visualize_barchart(top_n_topics=10)</b>
</div>

```python
model.visualize_barchart(top_n_topics=10)
```

![barchart1](/assets/images/barchart1.png)

<div style="text-align: center;">
- Display the top 5-10 words in a bar graph for each topic.
</div>

<div style="text-align: center;">
- The visual comparison of key word distributions by topic makes interpretation easier.
</div>

<br>

<div style="text-align: center;">
<b>>Visualize the distance between topics (Intertopic Map)</b>
</div>

```python
model.visualize_topics()
```

![intertopic_map](/assets/images/intertopic_map.png)


<br>

<div style="text-align: center;">
- 2D distance-based visualization (UMAP projection) between topics.
</div>

<div style="text-align: center;">
- The size of the circle is the importance of the topic, and the distance (interval) is semantic similarity.
</div>

<div style="text-align: center;">
- It allows you to see at a glance how much similar topics overlap or are separated.
</div>

<br>

<div style="text-align: center;">
<b>Visualize topic hierarchy (Dendrogram)</b>
</div>

```python
model.visualize_hierarchy(top_n_topics=20)
```

![Dendrogram1](/assets/images/Dendrogram1.png)

<div style="text-align: center;">
- Based on the similarity between topics, hierarchical cluster structure is represented.
</div>

<div style="text-align: center;">
- You can explore the possibility of top/bottom structures or mergers between topics.
</div>

<br>

<div style="text-align: center;">
<b>Coherence Score Calculation for Bertopic</b>
</div>

```python
top_n_words = 20
topic_words = []


topics_dict = model.get_topics() 

for topic_id, words_and_scores in topics_dict.items():
    if topic_id == -1: 
        continue
    words = [word for word, score in words_and_scores[:top_n_words]]
    topic_words.append(words)

texts_for_coherence = df_2_star['lemmas'].tolist()
dictionary = Dictionary(texts_for_coherence)
corpus = [dictionary.doc2bow(text) for text in texts_for_coherence]


coherence_model_bertopic = CoherenceModel(topics=topic_words,
                                          texts=texts_for_coherence,
                                          dictionary=dictionary,
                                          corpus=corpus,
                                          coherence='c_v')
coherence_bertopic = coherence_model_bertopic.get_coherence()
print(f"BERTopic Coherence Score (N={top_n_words}): {coherence_bertopic:.4f}")
```

![coherence2](/assets/images/coherence2.png)

<div style="text-align: center;">
- Evaluation of semantic consistency of words extracted from each topic.
</div>

<div style="text-align: center;">
- It quantitatively measures how logically a topic is tied to a person's view.
</div>

<div style="text-align: center;">
- Here, we show BERTopic's Coherence Score = 0.3574.
</div>

<br>

<div style="text-align: center;">
<b>Diversity Score Calculation for Bertopic</b>
</div>


```python
all_words = [word for topic in topic_words for word in topic]

total_words_count = len(all_words)
unique_words_count = len(set(all_words))
diversity_bertopic = unique_words_count / total_words_count

print(f"BERTopic Diversity Score (N={top_n_words}): {diversity_bertopic:.4f}")
```

![diversity2](/assets/images/diversity2.png)

<div style="text-align: center;">
- Calculate how many different words come out without duplication when collecting top-N words from all topics.
</div>


<div style="text-align: center;">
- By diagnosing the redundancy between topics, you can evaluate whether various topics are well separated.
</div>

<div style="text-align: center;">
- Here, Diversity Score = 0.7579 measured which is lower than LDA
</div>

<br>

<div style="text-align: center;">
<b>LLM classify</b>
</div>

```python
df_llm = df_2_star.sample(n=100, random_state=2025)
```

<div style="text-align: center;">
- Only 100 of the total reviews were randomly sampled.
</div>

<br>

<div style="text-align: center;">
<b>GPT API Calls</b>
</div>


```python
api_key = "Your api key"
output_col = "Keywords"

# GPT 
def gpt_prompt(prompt):
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    data = {
        "model": "gpt-3.5-turbo", 
        "messages": [
            {"role": "system", "content": "You extract keywords for analyzing customer reviews"},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.3
    }
    response = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, json=data)
    response.raise_for_status()
    result = response.json()["choices"][0]["message"]["content"].strip()
    result = " ".join(result.splitlines()).strip()
    if any(bad in result.lower() for bad in ["text"]):
        return "LLM Error"
    return result

```

<div style="text-align: center;">
- Invoke OpenAI GPT-3.5 to extract keywords based on prompt.
</div>

<br>

<div style="text-align: center;">
<b>Defining prompts for keyword extraction</b>
</div>

```python
def extract_keyphrases(text, index=None, total=None):
    if index is not None and total is not None:
        print(f"{index + 1} / {total}")

    prompt = f"""Analyze the following customer review and extract 5 to 10 key phrases that best represent the core topics, features, sentiments, and experiences mentioned in the review.

    Key phrases should capture the main subjects, specific product or service attributes, common issues, or positive aspects.

    Guidelines:
    - Each extracted phrase must clearly represent a specific point or idea from the review.
    - Formulate them as meaningful phrases, not just single words or a list of adjectives/verbs.
    - For example: "poor battery life", "excellent customer support", "difficult assembly process".
    - Output should be a single line, with key phrases separated by commas.
    - DO NOT include explanatory sentences, adjectives, adverbs, verbs, or full sentences.
    - The extracted key phrases will be used for subsequent document clustering and topic summarization.

    [Customer Review]
    {text}

    Key Phrases:"""
    return gpt_prompt(prompt)
```

<div style="text-align: center;">
- Ask GPT to extract 5-10 key keyword phrases from a given review.
</div>

<div style="text-align: center;">
- To extract only key information for topic modeling and use it for subsequent analysis.
</div>

<br>

<div style="text-align: center;">
Execute keyword extraction per review
</div>

```python
sub_df = df_llm.reset_index().copy()
total = len(sub_df)

for i, row in sub_df.iterrows():
    text = str(row['text'])
    result = extract_keyphrases(text, index=i, total=total)
    sub_df.at[i, output_col] = result  
    time.sleep(5)
```

<div style="text-align: center;">
- Keyword requests from GPT for each of the 100 reviews.
</div>

<div style="text-align: center;">
- Prevent API abuse by placing a 5-second delay.
</div>

<div style="text-align: center;">
- Perform LLM-based feature generation for each document.
</div>

<br>

<div style="text-align: center;">
<b>Hallucination Filtering</b>
</div>

```python
results = []
model = SentenceTransformer("sentence-transformers/paraphrase-MiniLM-L6-v2")


for idx, row in sub_df.iterrows():
    text = str(row['text'])  # original text or pre-processed text
    keyphrases = str(row['Keywords']).split(',')

    # embedding
    text_embedding = model.encode(text, normalize_embeddings=True)
    kp_embeddings = model.encode(keyphrases, normalize_embeddings=True)

    # similarity score
    sims = [np.dot(text_embedding, kp) for kp in kp_embeddings]
    sims_score = np.mean(sims)

    # filtering
    valid_kps = [kp for kp, score in zip(keyphrases, sims) if score >= 0.10]  # control threshold 

    results.append({
        'original_keyphrases': keyphrases,
        'coherence_score': sims_score,
        'valid_keyphrases': valid_kps,
    })
```

<div style="text-align: center;">
- The semantic similarity between each keyword and the original review is calculated (cosine similarity).
</div>

<div style="text-align: center;">
- Only keywords with similarity greater than 0.1 are considered valid.
</div>

<div style="text-align: center;">
- Remove false or irrelevant keywords generated by LLM and ensure reliability.
</div>

<br>

<div style="text-align: center;">
<b>Keyword embeddings and clustering</b>
</div>

```python
sub_df['embedding'] = sub_df['valid_keyphrases'].apply(embed_text)
embeddings = np.vstack(sub_df['embedding'].values)
n_clusters = 5  
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
```

```python
cluster_docs = (
    sub_df[sub_df["cluster"] != -1]
      .explode("valid_keyphrases")               
      .groupby("cluster")["valid_keyphrases"]
      .apply(lambda s: " ".join(s.astype(str)))   
      .to_dict()
)

vectorizer = TfidfVectorizer(max_features=1000)
top_terms_per_cluster = {}

for cluster_id, text in cluster_docs.items():
    tfidf = vectorizer.fit_transform([text])
    feature_array = np.array(vectorizer.get_feature_names_out())
    tfidf_sorting = np.argsort(tfidf.toarray()).flatten()[::-1]
    top_terms = feature_array[tfidf_sorting][:20]
    top_terms_per_cluster[cluster_id] = top_terms.tolist()

print("\n Top keywords by cluster:\n")
for cid, terms in top_terms_per_cluster.items():
    print(f"Cluster {cid}: {' / '.join(terms)}")
```

<div style="text-align: center;">
- Transform filtered keywords into sentence embeddings.
</div>

<div style="text-align: center;">
- Group documents into five clusters with the KMeans algorithm.
</div>

![keyword](/assets/images/keyword.png)

<div style="text-align: center;">
- Subject similarity-based clustering between documents based on semantic keywords extracted by LLM.
</div>

<br>

<div style="text-align: center;">
<b>Coherence Score & Diversity Score for llm</b>
</div>

![coherence3](/assets/images/coherence3.png)

<div style="text-align: center;">
- This represents a moderate or higher level of consistency.
</div>

![diversity3](/assets/images/diversity3.png)

<div style="text-align: center;">
- It shows a fairly high degree of thematic diversity.
</div>

<div style="font-size: 36px; text-align: center;">
  <b>Conclusion</b>
</div>

<div style="text-align: center;">
- LDAs are traditional but have high word repeatability and low consistency.
</div>

<div style="text-align: center;">
- BERTopic is fast and visualized well, but some topics overlap.
</div>

<div style="text-align: center;">
- LLM + filtering based topic analysis is the best in terms of accuracy and interpretability.
</div>



